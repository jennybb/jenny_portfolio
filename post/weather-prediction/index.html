<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Weather Prediction | Jianing Cao</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Python | Hierarchical Agglomerative Clustering | Decision Tree | K-means">
    <meta name="generator" content="Hugo 0.79.0" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="https://jennybb.github.io/jenny_portfolio/dist/css/app.4fc0b62e4b82c997bb0041217cd6b979.css" rel="stylesheet">
    

    

    
      

    

    
    
    <meta property="og:title" content="Weather Prediction" />
<meta property="og:description" content="Python | Hierarchical Agglomerative Clustering | Decision Tree | K-means" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jennybb.github.io/jenny_portfolio/post/weather-prediction/" />
<meta property="article:published_time" content="2020-10-14T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-10-14T00:00:00+00:00" />
<meta itemprop="name" content="Weather Prediction">
<meta itemprop="description" content="Python | Hierarchical Agglomerative Clustering | Decision Tree | K-means">
<meta itemprop="datePublished" content="2020-10-14T00:00:00+00:00" />
<meta itemprop="dateModified" content="2020-10-14T00:00:00+00:00" />
<meta itemprop="wordCount" content="2693">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Weather Prediction"/>
<meta name="twitter:description" content="Python | Hierarchical Agglomerative Clustering | Decision Tree | K-means"/>

	
  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('https://jennybb.github.io/jenny_portfolio/images/weather.png');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://jennybb.github.io/jenny_portfolio/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Jianing Cao
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://jennybb.github.io/jenny_portfolio/contact/" title="CONTACT page">
              CONTACT
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://jennybb.github.io/jenny_portfolio/about/" title="MORE ABOUT ME page">
              MORE ABOUT ME
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="https://jennybb.github.io/jenny_portfolio/post/" title="PROJECTS page">
              PROJECTS
            </a>
          </li>
          
        </ul>
      
      





<a href="https://www.instagram.com/jenny_cao__/" target="_blank" class="link-transition instagram link dib z-999 pt3 pt0-l mr1" title="Instagram link" rel="noopener" aria-label="follow on Instagram——Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.271,26.578v-0.006c0.502,0,1.005,0.01,1.508-0.002  c0.646-0.017,1.172-0.57,1.172-1.217c0-0.963,0-1.927,0-2.89c0-0.691-0.547-1.24-1.236-1.241c-0.961,0-1.922-0.001-2.883,0  c-0.688,0.001-1.236,0.552-1.236,1.243c-0.001,0.955-0.004,1.91,0.003,2.865c0.001,0.143,0.028,0.291,0.073,0.426  c0.173,0.508,0.639,0.82,1.209,0.823C41.344,26.579,41.808,26.578,42.271,26.578z M33,27.817c-3.384-0.002-6.135,2.721-6.182,6.089  c-0.049,3.46,2.72,6.201,6.04,6.272c3.454,0.074,6.248-2.686,6.321-6.043C39.254,30.675,36.462,27.815,33,27.817z M21.046,31.116  v0.082c0,4.515-0.001,9.03,0,13.545c0,0.649,0.562,1.208,1.212,1.208c7.16,0.001,14.319,0.001,21.479,0  c0.656,0,1.215-0.557,1.215-1.212c0.001-4.509,0-9.02,0-13.528v-0.094h-2.912c0.411,1.313,0.537,2.651,0.376,4.014  c-0.161,1.363-0.601,2.631-1.316,3.803s-1.644,2.145-2.779,2.918c-2.944,2.006-6.821,2.182-9.946,0.428  c-1.579-0.885-2.819-2.12-3.685-3.713c-1.289-2.373-1.495-4.865-0.739-7.451C22.983,31.116,22.021,31.116,21.046,31.116z   M45.205,49.255c0.159-0.026,0.318-0.049,0.475-0.083c1.246-0.265,2.264-1.304,2.508-2.557c0.025-0.137,0.045-0.273,0.067-0.409  V21.794c-0.021-0.133-0.04-0.268-0.065-0.401c-0.268-1.367-1.396-2.428-2.78-2.618c-0.058-0.007-0.113-0.02-0.17-0.03H20.761  c-0.147,0.027-0.296,0.047-0.441,0.08c-1.352,0.308-2.352,1.396-2.545,2.766c-0.008,0.057-0.02,0.114-0.029,0.171V46.24  c0.028,0.154,0.05,0.311,0.085,0.465c0.299,1.322,1.427,2.347,2.77,2.52c0.064,0.008,0.13,0.021,0.195,0.03H45.205z M33,64  C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>



<a href="https://www.linkedin.com/in/jianing-cao" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/jennybb" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>








    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        
          <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">Weather Prediction</h1>
          
            <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
              Python | Hierarchical Agglomerative Clustering | Decision Tree | K-means
            </h2>
          
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw9 center ph6">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        PROJECTS
      </aside>
      




  


      <h1 class="f1 athelas mt3 mb1">Weather Prediction</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-10-14T00:00:00Z">October 14, 2020</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-three-thirds-l"><p>The aim of this project is to compare different weather forecast models. Since our taget is whether this place will rain tomorrow, tt is a binary classification problem.  We will use unsupervised learning Kmeans and Hierarchical Agglomerative Clustering and supervised learbubg Decision Tree and Random Forest. The complete code can be find on my GitHub page <a href="https://github.com/jennybb/Weather-Prediction">https://github.com/jennybb/Weather-Prediction</a>.</p>
</br>
<h3 id="dataset">Dataset:</h3>
<p>This dataset contains daily weather observations from many locations across Australia. RainTomorrow is the target variable to predict. It means &ndash; did it rain the next day, Yes or No? This column is Yes if the rain for that day was 1mm or more.<br />
Information about the dataset (Weather Forecast Training.csv):</p>
<ul>
<li><b>Location:</b> The location name of the weather station</li>
<li><b>MinTemp:</b> The minimum temperature in degrees celsius</li>
<li><b>MaxTemp:</b> The maximum temperature in degrees celsius</li>
<li><b>Rainfall:</b> The amount of rainfall recorded for the day in mm</li>
<li><b>Evaporation:</b> The so-called Class A pan evaporation (mm) in the 24 hours to 9am</li>
<li><b>Sunshine: </b>The number of hours of bright sunshine in the day.</li>
<li><b>WindGustDir:</b> The direction of the strongest wind gust in the 24 hours to midnight</li>
<li><b>WindGustSpeed:</b> The speed (km/h) of the strongest wind gust in the 24 hours to midnight</li>
<li><b>WindDir:</b> Direction of the wind</li>
<li><b>WindSpeed:</b> Wind speed (km/hr) averaged over 10 minutes</li>
<li><b>Humidity:</b> Humidity (percent)</li>
<li><b>Pressure:</b> Atmospheric pressure (hpa) reduced to mean sea level</li>
<li><b>Cloud:</b> Fraction of sky obscured by cloud This is measured in “oktas”, which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear sky whilst an 8 indicates that it is completely overcast.</li>
<li><b>Temp:</b> Temperature (degrees C)</li>
<li><b>RainTodayBoolean:</b> 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0</li>
<li><b>RainTomorrow:</b> The target variable. Did it rain tomorrow?<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_raw.png"/> 
</figure>
<br />
</br></li>
</ul>
<h3 id="exploratory-data-analysis-eda-and-outliers">Exploratory Data Analysis (EDA) and Outliers</h3>
<ul>
<li>
<p><b>Problems</b></p>
<ul>
<li>Wether our targer column data distribution is balanced? This will influence on how we select our evalution metrics.</li>
<li>In some columns, such as Evaporation, Sunshine, Cloud, there are too many missing values, so we can not simply fill them with mean or median.</li>
<li>The difference in scale for most of our attributes. Having 16 dimensions that contain different units of measurement(mm Rainfall, km/hr Wind, hpa Pressure etc..) can have devastating affects on how our distance-based functions operate.</li>
<li>This dataset contains both numerical and categorical data<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_des.png"/> 
</figure>
<br />
<br></br></li>
</ul>
</li>
<li>
<p><b>Target Column</b><br />
In this dataset, the distribution of the target column is balanced, each class occupies about 50%, so that we can use accuracy to evaluate the model later.<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_target.png"/> 
</figure>
<br />
<br></br></p>
</li>
<li>
<p><b>Other Columns</b><br />
Let&rsquo;s take a look at some random features in the dataset.<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_eda.png"/> 
</figure>
</p>
<ul>
<li>Rainfall is extremely right-skewed, which means most of days, there is 0-20 millimeters of rain.</li>
<li>Evaporation is also very right-skewed that evaporation happens almost every day about 0-10.</li>
<li>The range of Pressure is 970-1040. This will reduce the scale along the x-axis in a way that is much more digestable while using k-nearest neighbors to impute missing values into our model.</li>
<li>Using standard scalar to normalize the data.</li>
</ul>
</li>
</ul>
<p><figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_loc_1.png"/> 
</figure>
<br />
It seems like most of our data is collected from the south-east coast of Australia, and the distribution of RainTomorrow doesn&rsquo;t have too many difference. We can assume that the location in this dataset may not be a necessary predictor.<br />
<br></br></p>
<ul>
<li><b>Outliers</b><br />
Here, we will only deal with numerical variables. We replace the outliers with the 99th and 1th percentile.<br />
<br></br></li>
</ul>
<h3 id="train-test-split">Train Test Split</h3>
<p>We use 70% of data as a training dataset and 30% of data as a testing dataset</p>
<h4 id="data-preparation">Data Preparation</h4>
<p><b>1. Missing values</b></p>
<p>First, we use missingno package to visualize the missing values. This package provides a few graphs that let us visualize missing data from a different perspective. This can help us a lot in the handling of missing data. The missingno library is based on matplotlib hence all graphs generated by it&rsquo;ll be static.</p>
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_miss.png"/> 
</figure>

<p>White lines indicate missing values. “Evporation&quot;, &ldquo;Sunshine&rdquo; and “Cloud” columns are dominated by while lines as we expect. But, there is an interesting trend in the other columns that have missing values. They mostly have missing values in common rows. If a row has a missing value in “WindGustDir&quot; columns, it is likely to have missing values in “WindGustSpeed”, “WindDir”, “WindSpeed”, &ldquo;Pressure&rdquo; and “Humidity” columns. This is highly valuable information when handling missing values.<br />
The sparkline at right summarizes the general shape of the data completeness and points out the rows with the maximum and minimum nullity in the dataset.</p>
<p>Heatmaps are used to visualize correlation matrices which show the correlation of values between different columns. Missingno library also provides heatmaps that show if there is any correlation between missing values in different columns.</p>
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_miss_heat.png"/> 
</figure>

<p>Positive correlation is proportional to the level of darkness in blue as indicated by the bar on the right side. There are positive correlations at different levels between “Sunshine”, “Evaporation”, “WindGustSpeed”, “WindGustDir”, and “WindSpeed” columns. The highest correlation is between “RainToday” and “rainfall”, as well as &ldquo;WindGustSpeed&rdquo; and &ldquo;WindGustDir&rdquo; which is 1. This confirms our intuition from the missing values matrix as these columns have missing values in the same rows.</p>
<p>The heatmap here, if the correlation between two columns are very high, only can tell one feature can be explained by another one, but we can&rsquo;t say it is from another feature, which means they are correlated. Missing values in this dataset, like &ldquo;evaporation&rdquo;, &ldquo;sunshine&rdquo;, this may because at some cities, they don&rsquo;t have enough equipments to measure this value. So we can assume these features are random, they are not correlated to each other. Therefore, when we dealing with missing values, we decide to use MICE imputation.</p>
<ol>
<li>
<p>For categorical variables,  we will use the mode to replace the missing values. For the X_test dataset, we will use the mode in the X_train to fill the missing values. Since here we assume X_test is an unseen dataset, so it&rsquo;s impossible to get the mode of it. This helps to avoid data leakage.</p>
<ul>
<li>&lsquo;WindGustDir&rsquo; fill the missing values with &lsquo;W&rsquo;</li>
<li>&lsquo;WindDir&rsquo; fill the missing values with &lsquo;W&rsquo;</li>
<li>&lsquo;RainToday&rsquo; fill the missing values with &lsquo;No&rsquo;</li>
</ul>
</li>
<li>
<p>For numerical varaibales, we will use mean or median to clean it. First, we need to plot the distribution of each numerical variables. And the missing values in X_test, we will use the mode/mean/median in X_train to replace them.<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_dis.png"/> 
</figure>
</p>
<ul>
<li>
<p>For continues variables:<br />
When its distribution is normal distribution, we use <ins>median or mean</ins> to replace it.<br />
When it&rsquo;s right-skewed, median &lt; mean, we use <ins>median</ins> to replace it.<br />
When it&rsquo;s left-skewed, mean &lt; median, we use <ins>mean</ins> to replace it.</p>
</li>
<li>
<p>For discrete variables:<br />
We use <ins>mode</ins> to replace it.<br />
<br></br></p>
</li>
</ul>
</li>
</ol>
<p><b>2. Categorical to Numerical</b></p>
<ul>
<li>
<p><b>Boolean Data</b></p>
<p>For RainToday and RainTomorrow:<br />
If Rain = Yes, then transfer it to 1<br />
If Rain = No, then transfer it to 0</p>
</li>
<li>
<p><b>Other Categorical Variables</b><br />
Since clustering and decision tree won&rsquo;t be affected a lot by the methods of turning categorical variables to numerical variables. Here we choose to use the label encoder.<br />
<br></br></p>
</li>
</ul>
<p><b>3. Normalization</b><br />
Form the description of data above, we can see, data here are in different units, and the range of some data is very large. To make the model more reasonable, we use normalization to unify the format.<br />
<br></br></p>
<p><b>4. Visualization</b><br />
The correlation heatmap below show you thee correlation between different features and target column. Usually, we can use this heatmap to do the feature selection before modeling. For example, for regression models (linear regression / logistic regession&hellip;), it&rsquo;s better to avoid features with high correlation, because multicolinearity can yield solutions that are wildly varying and possibly numerically unstable. While for models like Random Forest, it can be good at detecting interactions between different features, but highly correlated features can mask these interactions.<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_cor.png"/> 
</figure>
</p>
<ul>
<li>For the three categorical features: Location_Types, WindGustDir_Types, WindDir_Types, the correlation between RainTomorrow is 0.015, 0.064, 0.032. It seems there is not too much relation with our target. So we will not consider these three columns when we do the analysis later.</li>
<li>The correlation between Temp and MinTemp and MaxTemp are 0.71 and 0.98, since we know the averge temperature of a day is in the range of min and max temperature.</li>
<li>The correlation between Evaporation and Temp is 0.74, which make sense that we always think the higher the temperature, the more the water will evaporate.</li>
<li>The correlation between Cloud and Sunshine is -0.82, when it&rsquo;s clouding it can not be sunny.</li>
<li>Correlation between Cloud and Humidity is 0.65, since humidity can also be called moisture, and cloud generation is close to the moisture in the air. Once the relative humidity reaches 100%, any further cooling results in net condensation and cloud formation.</li>
</ul>
<p>Since the correlation between MaxTemp and Temp is 0.96, which is super high, so we will the MaxTemp.</p>
<p><br></br></p>
<h3 id="machine-learning-models">Machine Learning Models</h3>
<h4 id="1-k-means">1. K-means</h4>
<p>K-means clustering involves specifying a desired number of clusters (K) and assigning each observation to one of the clusters. The clusters are determined such that the total within-cluster variation, summed over all KK clusters, is minimized. Within-cluster variation is defined as the squared Euclidean distance.  The general process of performing K-means clustering is as follows:</p>
<ul>
<li>Randomly assign each observation to one of the clusters. These will serve as the initial cluster assignments.</li>
<li>For each of the clusters, compute the cluster centroid. The cluster centroid is the mean of the observations assigned to the cluster.</li>
<li>Reassign each observation to the cluster whose centroid is the closest.</li>
<li>Continue repeating steps 2 &amp; 3 until the result no longer changes.</li>
</ul>
<h4 id="11-elbow-method">1.1 Elbow Method</h4>
<p>To tune the model, first we use Elbow Method to find the opitmal k.<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_elbow.png"/> 
</figure>
</p>
<ul>
<li>Elbow method is to run k-means clustering on the dataset for a range of values of k , and for each value of k calculate the sum of squared errors (SSE).</li>
<li>Once the distortion starts decreasing in a linear fashion we would like to conclude the optimal number of clusters, k.</li>
<li>This elbow curve demonstrates that the optimal number of cluster for this algorithm being = 2</li>
</ul>
<h4 id="12-grid-search">1.2 Grid Search</h4>
<p>After find the optimal k, we will Grid Search and K-fold cross validation to tune the hyper-parameter. The optimal parameters we found are:</p>
<ul>
<li>algorithm = &lsquo;auto&rsquo;<br />
This is automatically set to elkan given that our data is densely clustered together, if our data was organized sparsely than the full algorithm would be called</li>
<li>init = &lsquo;k-means++&rsquo;<br />
‘k-means++’ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.</li>
<li>n_init= 4<br />
Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.</li>
<li>max_iter = 10<br />
Maximum number of iterations of the k-means algorithm for a single run.</li>
<li>precompute_distances = False</li>
</ul>
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_k_matrix.png"/> 
</figure>

<p>It seems like, not matter the accuracy or other evaluation methods like precision and recall, their score is low. This is because our data is binary and the standard &ldquo;mean&rdquo; operation does not make much sense for binary or categorical data, so K-means isn&rsquo;t an appropriate model for binary variables.<br />
<br></br></p>
<h4 id="2-hac-hierarchical-architecture-clustering">2. HAC: Hierarchical Architecture Clustering</h4>
<p>There are two types of hierarchical clustering: Agglomerative and Divisive. In the former, data points are clustered using a bottom-up approach starting with individual data points, while in the latter top-down approach is followed where all the data points are treated as one big cluster and the clustering process involves dividing the one big cluster into several small clusters.<br />
In this report we will focus on agglomerative clustering that involves the bottom-up approach.</p>
<p>Now clusters usually have multiple points in them that require a different approach for the distance matrix calculation. Linkage decides how the distance between clusters, or point to cluster distance is computed. Commonly used linkage mechanisms are outlined below:</p>
<ul>
<li><ins>Single Linkage</ins><br />
Distances between the most similar members for each pair of clusters are calculated and then clusters are merged based on the shortest distance</li>
<li><ins>Average Linkage</ins><br />
Distance between all members of one cluster is calculated to all other members in a different cluster. The average of these distances is then utilized to decide which clusters will merge</li>
<li><ins>Complete Linkage</ins><br />
Distances between the most dissimilar members for each pair of clusters are calculated and then clusters are merged based on the shortest distance</li>
<li><ins>Median Linkage</ins><br />
Similar to the average linkage, but instead of using the average distance, we utilize the median distance</li>
<li><ins>Ward Linkage</ins><br />
Uses the analysis of variance method to determine the distance between clusters</li>
<li><ins>Centroid Linkage</ins><br />
Calculates the centroid of each cluster by taking the average of all points assigned to the cluster and then calculates the distance to other clusters using this centroid</li>
</ul>
<p>The linkage method takes the dataset and the method to minimize distances as parameters. We use &lsquo;ward&rsquo; as the method since it minimizes then variants of distances between the clusters.</p>
<h4 id="22-create-dendrogram">2.2 Create Dendrogram</h4>
<p>There are no statistical techniques to decide the number of clusters in hierarchical clustering, unlike a K Means algorithm that uses an elbow plot to determine the number of clusters. However, one common approach is to analyze the dendrogram and look for groups that combine at a higher dendrogram distance. Let’s take a look at the example below.<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_hac.png"/> 
</figure>
<br />
The plot above illustrates the presence of 2 clusters when the tree is cut at a Dendrogram distance of 2700. The general idea being, all 2 groups of clusters combines at a much higher dendrogram distance and hence can be treated as individual groups for this analysis. We can also verify the same using a silhouette index score.<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_hac_matrix1.png"/> 
</figure>
<br />
<br></br></p>
<h4 id="3-decision-tree">3. Decision Tree</h4>
<p>Decision tree has two types: regression and classification. Since our target are binary variables, here is a classification problem. While classification trees are similar to regression trees, but predict qualitative responses. In regression, the prediction for some observation is given by the mean response of the observations at some terminal node. In classification, the prediction is given by the most commonly occurring class at the terminal node.</p>
<p>In classification, instead of using the RSS, we look at either the Gini index or the cross-entropy. Both are very similar measures, where small values indicate node purity, meaning that a node contains predominantly observations from a single class. Either of these measures can be used to build a tree and evaluate splits.</p>
<p>When it comes to pruning a tree, we could either use the Gini index, cross-entropy, or the classification error rate. The classification error rate is the fraction of observations in a region that do not belong to the most common class. The classification error rate may be preferable for the purposes of prediction accuracy and tree pruning.</p>
<p>The best model parameters are:</p>
<ul>
<li>criterion: gini</li>
<li>max_depth: 5</li>
<li>max_leaf_nodes: 20</li>
<li>min_samples_leaf: 1</li>
<li>min_samples_split: 2<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_dt.png"/> 
</figure>
</li>
</ul>
<p>Here is the performance of decision tree:<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_dt_matrix.png"/> 
</figure>
<br />
<br></br></p>
<h4 id="3-random-forest">3. Random Forest</h4>
<p>Random Forest is a specific type of decision tree. A decision tree is built on an entire dataset, using all the features/variables of interest, whereas a random forest randomly selects observations/rows and specific features/variables to build multiple decision trees from and then averages the results.</p>
<p>The best model parameters are:</p>
<ul>
<li><ins>max_features: sqrt</ins><br />
Max number of features considered for splitting a node</li>
<li><ins>min_samples_leaf: 2</ins><br />
Min number of data points allowed in a leaf node</li>
<li><ins>min_samples_split: 5</ins><br />
Min number of data points placed in a node before the node is split</li>
<li><ins>n_estimators: 72</ins><br />
Number of trees in the foreset</li>
</ul>
<p><figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_rf_matrix.png"/> 
</figure>
<br />
Compared with Decision Tree above, the performance of Random Forest is a little bit better. Since, in Random Forest, we use bagging to train the model. Bagging is a general purpose method for reducing the variance of a statistical learning method. It is very useful for decision trees because they suffer from high variance. Another advantage of Random Forest is that, we can generate the feature importance, which will give us a general view of the features in the dataset like which is important.<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_rf_imp.png"/> 
</figure>
<br />
It&rsquo;s easy to tell for the feature importance plot above, humidity plays a very important role when predicing whether it will rain tomorrow, which makes sense, since humidity measures the water in the air.<br />
<br></br></p>
<h3 id="models-comparison">Models Comparison</h3>
<p>In the table below, I concluded the performance of 4 models above.<br />
<figure>
    <img src="https://jennybb.github.io/jenny_portfolio/images/weather_master.png"/> 
</figure>
<br />
In this project, if we want to choose a model for prediction, Random Forest has the best performance, with the accuracy of 79.77%.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://jennybb.github.io/jenny_portfolio" >
    &copy;  Jianing Cao 2021 
  </a>
    <div>





<a href="https://www.instagram.com/jenny_cao__/" target="_blank" class="link-transition instagram link dib z-999 pt3 pt0-l mr1" title="Instagram link" rel="noopener" aria-label="follow on Instagram——Opens in a new window">
  <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M42.271,26.578v-0.006c0.502,0,1.005,0.01,1.508-0.002  c0.646-0.017,1.172-0.57,1.172-1.217c0-0.963,0-1.927,0-2.89c0-0.691-0.547-1.24-1.236-1.241c-0.961,0-1.922-0.001-2.883,0  c-0.688,0.001-1.236,0.552-1.236,1.243c-0.001,0.955-0.004,1.91,0.003,2.865c0.001,0.143,0.028,0.291,0.073,0.426  c0.173,0.508,0.639,0.82,1.209,0.823C41.344,26.579,41.808,26.578,42.271,26.578z M33,27.817c-3.384-0.002-6.135,2.721-6.182,6.089  c-0.049,3.46,2.72,6.201,6.04,6.272c3.454,0.074,6.248-2.686,6.321-6.043C39.254,30.675,36.462,27.815,33,27.817z M21.046,31.116  v0.082c0,4.515-0.001,9.03,0,13.545c0,0.649,0.562,1.208,1.212,1.208c7.16,0.001,14.319,0.001,21.479,0  c0.656,0,1.215-0.557,1.215-1.212c0.001-4.509,0-9.02,0-13.528v-0.094h-2.912c0.411,1.313,0.537,2.651,0.376,4.014  c-0.161,1.363-0.601,2.631-1.316,3.803s-1.644,2.145-2.779,2.918c-2.944,2.006-6.821,2.182-9.946,0.428  c-1.579-0.885-2.819-2.12-3.685-3.713c-1.289-2.373-1.495-4.865-0.739-7.451C22.983,31.116,22.021,31.116,21.046,31.116z   M45.205,49.255c0.159-0.026,0.318-0.049,0.475-0.083c1.246-0.265,2.264-1.304,2.508-2.557c0.025-0.137,0.045-0.273,0.067-0.409  V21.794c-0.021-0.133-0.04-0.268-0.065-0.401c-0.268-1.367-1.396-2.428-2.78-2.618c-0.058-0.007-0.113-0.02-0.17-0.03H20.761  c-0.147,0.027-0.296,0.047-0.441,0.08c-1.352,0.308-2.352,1.396-2.545,2.766c-0.008,0.057-0.02,0.114-0.029,0.171V46.24  c0.028,0.154,0.05,0.311,0.085,0.465c0.299,1.322,1.427,2.347,2.77,2.52c0.064,0.008,0.13,0.021,0.195,0.03H45.205z M33,64  C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>



<a href="https://www.linkedin.com/in/jianing-cao" target="_blank" class="link-transition linkedin link dib z-999 pt3 pt0-l mr1" title="LinkedIn link" rel="noopener" aria-label="follow on LinkedIn——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>


<a href="https://github.com/jennybb" target="_blank" class="link-transition github link dib z-999 pt3 pt0-l mr1" title="Github link" rel="noopener" aria-label="follow on Github——Opens in a new window">
  <svg  height="32px"  style="enable-background:new 0 0 512 512;" version="1.1" viewBox="0 0 512 512" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
  <path d="M256,32C132.3,32,32,134.8,32,261.7c0,101.5,64.2,187.5,153.2,217.9c11.2,2.1,15.3-5,15.3-11.1   c0-5.5-0.2-19.9-0.3-39.1c-62.3,13.9-75.5-30.8-75.5-30.8c-10.2-26.5-24.9-33.6-24.9-33.6c-20.3-14.3,1.5-14,1.5-14   c22.5,1.6,34.3,23.7,34.3,23.7c20,35.1,52.4,25,65.2,19.1c2-14.8,7.8-25,14.2-30.7c-49.7-5.8-102-25.5-102-113.5   c0-25.1,8.7-45.6,23-61.6c-2.3-5.8-10-29.2,2.2-60.8c0,0,18.8-6.2,61.6,23.5c17.9-5.1,37-7.6,56.1-7.7c19,0.1,38.2,2.6,56.1,7.7   c42.8-29.7,61.5-23.5,61.5-23.5c12.2,31.6,4.5,55,2.2,60.8c14.3,16.1,23,36.6,23,61.6c0,88.2-52.4,107.6-102.3,113.3   c8,7.1,15.2,21.1,15.2,42.5c0,30.7-0.3,55.5-0.3,63c0,6.1,4,13.3,15.4,11C415.9,449.1,480,363.1,480,261.7   C480,134.8,379.7,32,256,32z"/>
</svg>

<span class="new-window"><svg  height="8px"  style="enable-background:new 0 0 1000 1000;" version="1.1" viewBox="0 0 1000 1000" width="8px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" >
<path d="M598 128h298v298h-86v-152l-418 418-60-60 418-418h-152v-86zM810 810v-298h86v298c0 46-40 86-86 86h-596c-48 0-86-40-86-86v-596c0-46 38-86 86-86h298v86h-298v596h596z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>
</span></a>







</div>
  </div>
</footer>

    

  <script src="https://jennybb.github.io/jenny_portfolio/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
